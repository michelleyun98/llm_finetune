{"cells":[{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5034,"status":"ok","timestamp":1682675732836,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"},"user_tz":420},"id":"91467550-88d9-4c69-896f-5083e0f2822c","outputId":"6fcee859-9ce0-479d-93d6-215615acae9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.14.1)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.11.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.0+cu118)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft) (0.18.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.21.0)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"]}],"source":["!pip install transformers evaluate peft python-dotenv huggingface_hub wandb"],"id":"91467550-88d9-4c69-896f-5083e0f2822c"},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1051,"status":"ok","timestamp":1682675733880,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"},"user_tz":420},"id":"mr6RZMJ42hyQ","outputId":"41cbfc24-4175-4eac-ea59-d797f87512f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"mr6RZMJ42hyQ"},{"cell_type":"markdown","source":["**Imports**"],"metadata":{"id":"HGgchrQDaP6v"},"id":"HGgchrQDaP6v"},{"cell_type":"code","execution_count":149,"metadata":{"id":"xZRje8Gw2XcN","executionInfo":{"status":"ok","timestamp":1682677652220,"user_tz":420,"elapsed":332,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"outputs":[],"source":["import os\n","import csv\n","import json\n","import gc\n","\n","import torch\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n","import evaluate\n","from transformers import Trainer, TrainingArguments, GPT2DoubleHeadsModel, AutoTokenizer, GPT2LMHeadModel\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","import csv\n","import json\n","import pandas as pd\n","from collections import defaultdict\n","import re\n","import random\n","from random import shuffle\n","from sklearn.model_selection import train_test_split\n"],"id":"xZRje8Gw2XcN"},{"cell_type":"code","execution_count":103,"metadata":{"id":"MLHktVQ5trvJ","executionInfo":{"status":"ok","timestamp":1682675733881,"user_tz":420,"elapsed":13,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"outputs":[],"source":["import logging\n","logging.disable(logging.WARNING)"],"id":"MLHktVQ5trvJ"},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1596,"status":"ok","timestamp":1682675735464,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"},"user_tz":420},"id":"DK6CpGbr2oDJ","outputId":"35caa397-1860-4077-a0a6-f4702be41340"},"outputs":[{"output_type":"stream","name":"stdout","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["from dotenv import load_dotenv\n","load_dotenv(\"/content/drive/MyDrive/Colab_Notebooks/.env\")\n","HF = os.getenv(\"HF_TOKEN\")\n","WANDB = os.getenv(\"WANDB_TOKEN\")\n","\n","!huggingface-cli login --token $HF\n","!wandb login $WANDB"],"id":"DK6CpGbr2oDJ"},{"cell_type":"code","execution_count":147,"metadata":{"executionInfo":{"elapsed":3081,"status":"ok","timestamp":1682677341006,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"},"user_tz":420},"id":"985bcd1a-7219-43df-8f3e-c7f253dee244"},"outputs":[],"source":["MODEL_PATH = \"/content/drive/MyDrive/Colab_Notebooks/Brainy/ckpt/brainy-gpt-2/checkpoint-400\" ## A GPT2 Model pretrained for next-sentence prediction\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(MODEL_PATH) # Initialize for language modelling\n","\n","# Add special tokens\n","tokenizer.sep_token = \"[SEP]\"\n","tokenizer.cls_token = \"[CLS]\"\n","T = \"[THERA]\"\n","C = \"[CLI]\"\n","tokenizer.bos_token = \"<bos>\"\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_special_tokens({\"cls_token\": tokenizer.cls_token, \"sep_token\":tokenizer.sep_token, \"pad_token\":tokenizer.pad_token, \"bos_token\":tokenizer.bos_token, \"additional_special_tokens\":[T, C]})\n","embedding_layer = model.resize_token_embeddings(len(tokenizer))"],"id":"985bcd1a-7219-43df-8f3e-c7f253dee244"},{"cell_type":"code","execution_count":107,"metadata":{"id":"GqHcfx_sE-HM","executionInfo":{"status":"ok","timestamp":1682675776441,"user_tz":420,"elapsed":359,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"outputs":[],"source":["PATH = \"/content/drive/MyDrive/Colab_Notebooks/Brainy/\""],"id":"GqHcfx_sE-HM"},{"cell_type":"markdown","source":["**Read in data from CSV and preprocess to appropriate format**"],"metadata":{"id":"Y03tlytvaD_h"},"id":"Y03tlytvaD_h"},{"cell_type":"code","execution_count":108,"metadata":{"id":"hYuj_z39FEto","executionInfo":{"status":"ok","timestamp":1682675777496,"user_tz":420,"elapsed":202,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"outputs":[],"source":["exp = re.compile(r\"\\[.*\\]\")\n","def speechprocess(line):\n","    replacements = [\"I see\", \"I understand\", \"I hear you\"]\n","    sub = replacements[random.randint(0,2)]\n","    #disfluencies = re.compile(r\"um|uh[.,]?\", re.IGNORECASE)\n","    speechacts = re.compile(r\"hmm|mm-hmm\", re.IGNORECASE)\n","    line = speechacts.sub(sub, line, re.IGNORECASE)\n","    return line.strip(\" |-\")"],"id":"hYuj_z39FEto"},{"cell_type":"code","source":["dataset = defaultdict(list)\n","\n","with open(PATH + \"therapy_data.csv\", \"r\", encoding=\"utf-8\") as csvf:\n","    csvreader = csv.DictReader(csvf)\n","    for i, line in enumerate(csvreader):\n","        text =speechprocess(line[\"utterance_text\"]).strip(\"-\")\n","        if exp.match(text):\n","            continue\n","        if line[\"interlocutor\"] == \"therapist\":\n","            dataset[\"therapist_lines\"].append( f\"{T} \" + text.replace(r\"^h \", \"Okay\"))\n","        if line[\"interlocutor\"] == \"client\":\n","            dataset[\"client_lines\"].append(f\"{C} \" + text.replace(r\"^h \", \"Okay\"))\n","            \n","                "],"metadata":{"id":"fdkIsm5F8MzK","executionInfo":{"status":"ok","timestamp":1682675784368,"user_tz":420,"elapsed":222,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"id":"fdkIsm5F8MzK","execution_count":111,"outputs":[]},{"cell_type":"markdown","source":["**Datasets for Language Modelling** "],"metadata":{"id":"IpD5csNdZ7pt"},"id":"IpD5csNdZ7pt"},{"cell_type":"code","source":["replies = dataset[\"therapist_lines\"][:-40]\n","inputs = dataset[\"client_lines\"]\n","all_inputs = [(r,i) for r,i in zip(replies,inputs)]\n","\n","train, dev = train_test_split(all_inputs, random_state=0)\n","\n","train_replies, train_inputs = [i[0] for i in train], [i[1] for i in train]\n","dev_replies, dev_inputs = [i[0] for i in dev], [i[1] for i in dev]\n","\n","def get_input_data(replies,inputs):\n","  histories, ttids, labels, pos_ids = [], [], [], []\n","  for i, (th,cl) in enumerate(zip(replies, inputs)):\n","    if i == 0:\n","      continue\n","    if i % 3 == 0:\n","      prev_thera, prev_client, next_thera = tokenizer.encode(replies[i-3]), tokenizer.encode(inputs[i-2]), tokenizer.encode(replies[i-1])\n","      history = [tokenizer.bos_token_id] + prev_thera + prev_client + next_thera + [tokenizer.eos_token_id]\n","      pos_id = list(range(len(history)))\n","      ttid = [0]*(len(prev_thera)+1) + [1]*len(prev_client) + [0] *(len(next_thera)+1)\n","\n","      tokenized_lm_target = next_thera + [tokenizer.eos_token_id]\n","      padding = [-100] * (len(history) - len(tokenized_lm_target))\n","      label = padding + tokenized_lm_target\n","\n","      assert len(ttid) == len(history) == len(label) == len(pos_id)\n","      histories.append(history)\n","      ttids.append(ttid)\n","      labels.append(label)\n","      pos_ids.append(pos_id)\n","  return histories, ttids, labels, pos_ids\n","\n","class BrainyData(Dataset):\n","  def __init__(self, inputs, ttids, labels,posids):\n","    self.input = inputs\n","    self.ids = ttids\n","    self.lm_labels = labels\n","    self.posids = posids\n","  def __len__(self):\n","    return len(self.input)\n","  def __getitem__(self, index):\n","    input = self.input[index]\n","    id = self.ids[index]\n","    lm_label = self.lm_labels[index]\n","    pos_id = self.posids[index]\n","\n","    example = {\"input_ids\":input, \"token_type_ids\":id.squeeze(), \"labels\":lm_label, \"position_ids\":pos_id.squeeze()}\n","    return example\n","\n","def convert_to_tensor(replies, inputs):\n","  histories, ttids, labels,posids = get_input_data(replies, inputs)\n","  \n","  input_tensors, tt_id_tensors, label_tensors, pos_id_tensors = [], [], [], []\n","  for h,t,l,p in zip(histories, ttids, labels,posids):\n","    input_tensors.append(torch.tensor(h))\n","    tt_id_tensors.append(torch.tensor(t))\n","    label_tensors.append(torch.tensor(l))\n","    pos_id_tensors.append(torch.tensor(p))\n","    \n","\n","  return BrainyData(input_tensors,tt_id_tensors, label_tensors, pos_id_tensors)\n","  \n","train_data=convert_to_tensor(train_replies, train_inputs)\n","dev_data=convert_to_tensor(dev_replies, dev_inputs)\n"],"metadata":{"id":"wgyn_yVn55dZ","executionInfo":{"status":"ok","timestamp":1682675785915,"user_tz":420,"elapsed":235,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"id":"wgyn_yVn55dZ","execution_count":113,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)\n","\n","model_name = \"brainy-gpt-4.1\"\n","SAVE_DIR = \"/content/drive/MyDrive/Colab_Notebooks/Brainy/ckpt/\"\n","\n","training_args = TrainingArguments(\n","  output_dir = SAVE_DIR + f\"{model_name}\",\n","  log_level = \"error\",\n","  num_train_epochs = 10,\n","  learning_rate = 5e-4,\n","  lr_scheduler_type = \"linear\",\n","  warmup_steps = 90,\n","  optim = \"adamw_torch\",\n","  weight_decay = 0.01,\n","  per_device_train_batch_size = 1,\n","  per_device_eval_batch_size = 1,\n","  gradient_accumulation_steps = 16,\n","  evaluation_strategy = \"epoch\",\n","  eval_steps = 100,\n","  logging_steps = 10,\n","  push_to_hub = False\n",")\n","\n","trainer = Trainer(\n","  model = model,\n","  args = training_args,\n","  train_dataset = train_data,\n","  eval_dataset = dev_data,\n","  data_collator = data_collator,\n","  tokenizer = tokenizer,\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"19qitGoK_Kah","executionInfo":{"status":"error","timestamp":1682676573376,"user_tz":420,"elapsed":782404,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}},"outputId":"8fa2245b-fbe5-4ad4-dd2f-9f93ff0684cd"},"id":"19qitGoK_Kah","execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["{'loss': 10.4503, 'learning_rate': 5.555555555555555e-05, 'epoch': 0.31}\n","{'loss': 3.9219, 'learning_rate': 0.0001111111111111111, 'epoch': 0.63}\n","{'loss': 2.7411, 'learning_rate': 0.00016666666666666666, 'epoch': 0.94}\n","{'eval_loss': 2.9260401725769043, 'eval_runtime': 9.6004, 'eval_samples_per_second': 17.708, 'eval_steps_per_second': 17.708, 'epoch': 0.97}\n","{'loss': 2.2945, 'learning_rate': 0.0002222222222222222, 'epoch': 1.25}\n","{'loss': 2.159, 'learning_rate': 0.0002777777777777778, 'epoch': 1.57}\n","{'loss': 2.0441, 'learning_rate': 0.0003333333333333333, 'epoch': 1.88}\n","{'eval_loss': 2.8131580352783203, 'eval_runtime': 8.993, 'eval_samples_per_second': 18.904, 'eval_steps_per_second': 18.904, 'epoch': 1.97}\n","{'loss': 1.7804, 'learning_rate': 0.0003888888888888889, 'epoch': 2.19}\n","{'loss': 1.7001, 'learning_rate': 0.0004444444444444444, 'epoch': 2.5}\n","{'loss': 1.7903, 'learning_rate': 0.0005, 'epoch': 2.82}\n","{'eval_loss': 2.963383674621582, 'eval_runtime': 9.2594, 'eval_samples_per_second': 18.36, 'eval_steps_per_second': 18.36, 'epoch': 2.97}\n","{'loss': 1.5469, 'learning_rate': 0.0004772727272727273, 'epoch': 3.13}\n","{'loss': 1.3764, 'learning_rate': 0.00045454545454545455, 'epoch': 3.44}\n","{'loss': 1.4441, 'learning_rate': 0.0004318181818181818, 'epoch': 3.76}\n","{'eval_loss': 3.0961010456085205, 'eval_runtime': 9.876, 'eval_samples_per_second': 17.213, 'eval_steps_per_second': 17.213, 'epoch': 3.98}\n","{'loss': 1.396, 'learning_rate': 0.00040909090909090913, 'epoch': 4.07}\n","{'loss': 1.1248, 'learning_rate': 0.00038636363636363635, 'epoch': 4.38}\n","{'loss': 1.2063, 'learning_rate': 0.00036363636363636367, 'epoch': 4.7}\n","{'eval_loss': 3.4150192737579346, 'eval_runtime': 8.8797, 'eval_samples_per_second': 19.145, 'eval_steps_per_second': 19.145, 'epoch': 4.98}\n","{'loss': 1.1172, 'learning_rate': 0.0003409090909090909, 'epoch': 5.01}\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 34>\u001b[0m:\u001b[94m34\u001b[0m                                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1929 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2717\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2714 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2715 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2716 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2717 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2718 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2719 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2720 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 34&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">34</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2717</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2714 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2715 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2716 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2718 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2719 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2720 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model.push_to_hub(\"michelleyunun/brainy-lm-2\")"],"metadata":{"id":"8JR1l7gtbVu6"},"id":"8JR1l7gtbVu6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Datasets for Classification (Next-Sentence Prediction)"],"metadata":{"id":"tXapJLZnZZHT"},"id":"tXapJLZnZZHT"},{"cell_type":"code","execution_count":152,"metadata":{"id":"LPkkMXQ3f8FQ","executionInfo":{"status":"ok","timestamp":1682677919301,"user_tz":420,"elapsed":1484,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"outputs":[],"source":["distraction_lines = []\n","with open(PATH + \"WikiQA-train.tsv\", \"r\") as tsvf:\n","    for i, line in enumerate(tsvf.readlines()):\n","        if i == 0:\n","            continue\n","        if i % 5 == 0:\n","            distraction_lines.append(line.split(\"\\t\")[-2] + f\" {tokenizer.cls_token}\")\n","random.shuffle(distraction_lines) # shuffle to get rid of related lines\n","\n","gold_replies = dataset[\"therapist_lines\"]\n","train_gold, val_gold = train_test_split(gold_replies, random_state=0)\n","train_distract, val_distract = train_test_split(distraction_lines,  random_state=0)\n","test_gold, test_distract = val_gold[:100], val_distract[:300]\n","\n","from copy import deepcopy\n","\n","def get_mc_inputs_and_ids(gold_replies, distract_replies):\n","    mc_inputs = []\n","    mc_labels = []\n","    for i in range(len(gold_replies)-1):\n","      gold_reply = gold_replies[i]\n","\n","      mc_label = random.randint(0, 1)\n","      mc_labels.append([mc_label])\n","      distractors = distraction_lines[i:i+2]\n","    \n","      distractors[mc_label] = gold_reply\n","      mc_input = deepcopy(distractors)\n","\n","      mc_inputs.append(mc_input)\n","\n","    return mc_inputs,mc_labels      \n","\n","class BrainyData(Dataset):\n","  def __init__(self, inputs, ids, mc_labels, lm_labels, masks):\n","    self.input = inputs\n","    self.ids = ids\n","    self.mc_labels = mc_labels\n","    self.lm_labels = lm_labels\n","    self.masks = masks\n","  def __len__(self):\n","    return len(self.input)\n","  def __getitem__(self, index):\n","    input = self.input[index]\n","    id = self.ids[index]\n","    mc_label = self.mc_labels[index]\n","    mask = self.masks[index]\n","    lm_label = self.lm_labels[index]\n","\n","    example = {\"input_ids\":input, \"attention_mask\":mask, \"mc_token_ids\":id.squeeze(), \"mc_labels\":mc_label, \"labels\":lm_label}\n","    return example\n","\n","def convert_to_tensor(inputs, labels):\n","  inputs,labels = get_mc_inputs_and_ids(inputs, labels)\n","  \n","  input_tensors, id_tensors, label_tensors, attention_mask, lm_label,context_lens = [], [], [], [], [], []\n","  for inp,lab in zip(inputs,labels):\n","    ids = torch.tensor([[len(i.split())-1 for i in inp]])\n","\n","    encoded_targets = pad_sequence([torch.tensor(tokenizer.encode(target)) for target in inp], batch_first=True, padding_value=tokenizer.pad_token_id)\n","    lm_target = torch.full(encoded_targets.shape,torch.tensor(-100))\n","    lm_target[lab] = encoded_targets[lab]\n","\n","    mask=encoded_targets!=tokenizer.pad_token_id\n","    lm_label.append(lm_target)\n","    id_tensors.append(ids)\n","    input_tensors.append(encoded_targets)\n","    attention_mask.append(mask.long())\n","    label_tensors.append(torch.tensor(lab))\n","\n","  return BrainyData(input_tensors,id_tensors, label_tensors, lm_label, attention_mask)\n","\n","mc_train_data = convert_to_tensor(train_gold, train_distract)\n","mc_val_data = convert_to_tensor(val_gold, val_distract)\n","mc_test_data = convert_to_tensor(test_gold, test_distract)"],"id":"LPkkMXQ3f8FQ"},{"cell_type":"code","source":["## Load model (now finetuned for language modelling on therapy transcripts) for further tuning on NSP\n","model = GPT2LMHeadModel.from_pretrained(\"michelleyunun/brainy-lm-2\")"],"metadata":{"id":"8OBYe_FNZQ8w","executionInfo":{"status":"ok","timestamp":1682677878219,"user_tz":420,"elapsed":2152,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"id":"8OBYe_FNZQ8w","execution_count":151,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cp1uNsC5fInb","outputId":"3096887a-a189-4e43-f2da-61d571b4f2ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'loss': 4.6136, 'learning_rate': 5.555555555555555e-05, 'epoch': 0.1}\n","{'loss': 3.7075, 'learning_rate': 0.0001111111111111111, 'epoch': 0.2}\n","{'loss': 2.8564, 'learning_rate': 0.00016666666666666666, 'epoch': 0.31}\n","{'loss': 2.6114, 'learning_rate': 0.0002222222222222222, 'epoch': 0.41}\n","{'loss': 2.4255, 'learning_rate': 0.0002777777777777778, 'epoch': 0.51}\n","{'loss': 2.4714, 'learning_rate': 0.0003333333333333333, 'epoch': 0.61}\n","{'loss': 2.2504, 'learning_rate': 0.0003888888888888889, 'epoch': 0.72}\n","{'loss': 2.1473, 'learning_rate': 0.0004444444444444444, 'epoch': 0.82}\n","{'loss': 2.1767, 'learning_rate': 0.0005, 'epoch': 0.92}\n","{'loss': 1.9413, 'learning_rate': 0.0004898373983739837, 'epoch': 1.02}\n","{'eval_loss': 2.4188029766082764, 'eval_runtime': 29.164, 'eval_samples_per_second': 17.864, 'eval_steps_per_second': 17.864, 'epoch': 1.02}\n","{'loss': 1.3908, 'learning_rate': 0.0004796747967479675, 'epoch': 1.13}\n","{'loss': 1.4738, 'learning_rate': 0.0004695121951219512, 'epoch': 1.23}\n","{'loss': 1.4362, 'learning_rate': 0.00045934959349593497, 'epoch': 1.33}\n","{'loss': 1.5005, 'learning_rate': 0.0004491869918699187, 'epoch': 1.43}\n","{'loss': 1.4441, 'learning_rate': 0.00043902439024390245, 'epoch': 1.54}\n","{'loss': 1.3468, 'learning_rate': 0.00042886178861788616, 'epoch': 1.64}\n","{'loss': 1.489, 'learning_rate': 0.00041869918699186993, 'epoch': 1.74}\n","{'loss': 1.3731, 'learning_rate': 0.00040853658536585364, 'epoch': 1.84}\n","{'loss': 1.5047, 'learning_rate': 0.0003983739837398374, 'epoch': 1.94}\n","{'loss': 1.0912, 'learning_rate': 0.0003882113821138211, 'epoch': 2.05}\n","{'eval_loss': 2.137338876724243, 'eval_runtime': 30.5846, 'eval_samples_per_second': 17.035, 'eval_steps_per_second': 17.035, 'epoch': 2.05}\n","{'loss': 0.7107, 'learning_rate': 0.0003780487804878049, 'epoch': 2.15}\n","{'loss': 0.7326, 'learning_rate': 0.0003678861788617886, 'epoch': 2.25}\n","{'loss': 0.84, 'learning_rate': 0.00035772357723577237, 'epoch': 2.35}\n","{'loss': 0.8192, 'learning_rate': 0.0003475609756097561, 'epoch': 2.46}\n","{'loss': 0.7592, 'learning_rate': 0.00033739837398373985, 'epoch': 2.56}\n","{'loss': 0.762, 'learning_rate': 0.00032723577235772356, 'epoch': 2.66}\n","{'loss': 0.8581, 'learning_rate': 0.00031707317073170733, 'epoch': 2.76}\n","{'loss': 0.7685, 'learning_rate': 0.00030691056910569104, 'epoch': 2.87}\n","{'loss': 0.8406, 'learning_rate': 0.0002967479674796748, 'epoch': 2.97}\n","{'loss': 0.6074, 'learning_rate': 0.0002865853658536585, 'epoch': 3.07}\n","{'eval_loss': 2.1914377212524414, 'eval_runtime': 30.0981, 'eval_samples_per_second': 17.31, 'eval_steps_per_second': 17.31, 'epoch': 3.07}\n","{'loss': 0.5239, 'learning_rate': 0.0002764227642276423, 'epoch': 3.17}\n","{'loss': 0.5102, 'learning_rate': 0.000266260162601626, 'epoch': 3.28}\n","{'loss': 0.5333, 'learning_rate': 0.00025609756097560977, 'epoch': 3.38}\n","{'loss': 0.5633, 'learning_rate': 0.00024593495934959353, 'epoch': 3.48}\n"]}],"source":["model_name = \"brainy-gpt-4.1\"\n","SAVE_DIR = \"/content/drive/MyDrive/Colab_Notebooks/Brainy/ckpt/\"\n","\n","training_args = TrainingArguments(\n","  output_dir = SAVE_DIR + f\"{model_name}\",\n","  log_level = \"error\",\n","  num_train_epochs = 6,\n","  learning_rate = 5e-4,\n","  lr_scheduler_type = \"linear\",\n","  warmup_steps = 90,\n","  optim = \"adamw_torch\",\n","  weight_decay = 0.01,\n","  per_device_train_batch_size = 1,\n","  per_device_eval_batch_size = 1,\n","  gradient_accumulation_steps = 16,\n","  evaluation_strategy = \"steps\",\n","  eval_steps = 100,\n","  logging_steps = 10,\n","  push_to_hub = False\n",")\n","\n","trainer = Trainer(\n","  model = model,\n","  args = training_args,\n","  train_dataset = mc_train_data,\n","  eval_dataset = mc_val_data,\n","  data_collator = data_collator,\n","  tokenizer = tokenizer,\n",")\n","\n","trainer.train()"],"id":"cp1uNsC5fInb"},{"cell_type":"code","execution_count":117,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":649,"status":"ok","timestamp":1682676623179,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"},"user_tz":420},"id":"UxTwN87rr1GY","outputId":"4396cd3c-5ed7-4247-e282-76bc66ace78b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2958"]},"metadata":{},"execution_count":117}],"source":["gc.collect()"],"id":"UxTwN87rr1GY"},{"cell_type":"code","execution_count":118,"metadata":{"id":"NSUHzuj1FojB","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["904660d1ac2743d4b579f2c76e8c7356","5467123523fc4c4098ea5ff6439b2734","e08581a09ba241f28f89992cabe95a7c","a8394eb4ebf7468d8a7c390648d1cddd","cf497b659ac34f2e9935cf9cb1727891","b7513d8f6aa44daa877f5e838f0b9f8b","210a64745f444ec498c0a6d95c677f10","34b0fcae21cd420bb9a96c40ede1725d","7610943e7924465dbe786fc2dabb6023","834043616ae14116966e3abd2b336999","c797d39cb6b643cdb9097a702686977d","9300a1c995a9476394be020e840e2de6","28b93428e53044bd884b698dc966c484","3f984ce8655144be81b4a842da06d507","d0b349918ce2424fb61da2f844a84219","042a04c16c0d4aee9433658b9d3cfbb7","0c483230eabb40afa9b7b410fad487d7","9f97177644ff4ce6adeb858e0830c086","355e5da46bf54bb6a3e8e4c1e82049c6","dfb294c852e94afd8e24b3296c7d5bb0","010cd836077a4a4fbd4e5aaf824f15be","8fda36eca9c849ca90fabc0f5843d411","bab1447b5db048748182b65457429901","81db9a1051194faf9b10cddfd632a2b6","7372c8c4105d4ed78ad22b70260a7d00","85db9c6dced9484d94b5bf8ec46f4e5f","e5544f7da9884244bcef3b2b87852eac","1631c03ed99d478eb562cee893e0f181","f2738ce1dce349d6b3fcd1dfb9ff8873","206c6e2dee5f4dc5b8384d2ae26dee4c","703833a8526042c48f4813fefd992099","dc6238d4b6284a029d882cb0c8bdc335","a9fc8c0b135046bdacfe7f06095935a0"]},"executionInfo":{"status":"ok","timestamp":1682676640695,"user_tz":420,"elapsed":7735,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}},"outputId":"25f12bcb-ea40-42fa-e422-f635925abfec"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904660d1ac2743d4b579f2c76e8c7356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9300a1c995a9476394be020e840e2de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab1447b5db048748182b65457429901"}},"metadata":{}}],"source":["from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n","from transformers import AutoModel\n","brainy = GPT2LMHeadModel.from_pretrained(\"michelleyunun/brainy-lm-2\")"],"id":"NSUHzuj1FojB"},{"cell_type":"code","source":["inputs = tokenizer.encode(\"[CLI] I am feeling anxious. [THERA] When do you feel anxious? [CLI] When I'm around other people. [THERA]\", return_tensors=\"pt\")"],"metadata":{"id":"4_ZlRn7VlGav","executionInfo":{"status":"ok","timestamp":1682677163052,"user_tz":420,"elapsed":206,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}}},"id":"4_ZlRn7VlGav","execution_count":144,"outputs":[]},{"cell_type":"code","source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","\n","# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","sample_outputs = brainy.generate(\n","    inputs,\n","    do_sample=True, \n","    max_length=50, \n","    no_repeat_ngram_size=2, \n","    top_k=60, \n","    top_p=0.95, \n","    num_return_sequences=5\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, sample_output in enumerate(sample_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI1aXB_zjLA1","executionInfo":{"status":"ok","timestamp":1682677175324,"user_tz":420,"elapsed":1096,"user":{"displayName":"Michelle Yun","userId":"04387027356053324441"}},"outputId":"8cdee5fc-3d8b-47e0-bac3-3922cc930198"},"id":"wI1aXB_zjLA1","execution_count":146,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0:  I am feeling anxious.  When do you feel anxious?  When I'm around other people.  I see. So, in terms of how anxious you are, how important it is to sort of be in your head,\n","1:  I am feeling anxious.  When do you feel anxious?  When I'm around other people.  As you would a young woman? You would? I guess that would be the time when you might be feeling\n","2:  I am feeling anxious.  When do you feel anxious?  When I'm around other people.  In some ways, it makes me feel good. It makes you appreciate how much my role as your doctor plays a role for\n","3:  I am feeling anxious.  When do you feel anxious?  When I'm around other people.  Yeah. And how often do those people come to your house for a visit? It's like here, you know, after\n","4:  I am feeling anxious.  When do you feel anxious?  When I'm around other people.  And what do they bring you in? And, um, what I understand is that some people your age, or the\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"weNkr9hMjXZN"},"id":"weNkr9hMjXZN","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"904660d1ac2743d4b579f2c76e8c7356":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5467123523fc4c4098ea5ff6439b2734","IPY_MODEL_e08581a09ba241f28f89992cabe95a7c","IPY_MODEL_a8394eb4ebf7468d8a7c390648d1cddd"],"layout":"IPY_MODEL_cf497b659ac34f2e9935cf9cb1727891"}},"5467123523fc4c4098ea5ff6439b2734":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7513d8f6aa44daa877f5e838f0b9f8b","placeholder":"​","style":"IPY_MODEL_210a64745f444ec498c0a6d95c677f10","value":"Downloading (…)lve/main/config.json: 100%"}},"e08581a09ba241f28f89992cabe95a7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34b0fcae21cd420bb9a96c40ede1725d","max":1059,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7610943e7924465dbe786fc2dabb6023","value":1059}},"a8394eb4ebf7468d8a7c390648d1cddd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_834043616ae14116966e3abd2b336999","placeholder":"​","style":"IPY_MODEL_c797d39cb6b643cdb9097a702686977d","value":" 1.06k/1.06k [00:00&lt;00:00, 89.3kB/s]"}},"cf497b659ac34f2e9935cf9cb1727891":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7513d8f6aa44daa877f5e838f0b9f8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"210a64745f444ec498c0a6d95c677f10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34b0fcae21cd420bb9a96c40ede1725d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7610943e7924465dbe786fc2dabb6023":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"834043616ae14116966e3abd2b336999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c797d39cb6b643cdb9097a702686977d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9300a1c995a9476394be020e840e2de6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28b93428e53044bd884b698dc966c484","IPY_MODEL_3f984ce8655144be81b4a842da06d507","IPY_MODEL_d0b349918ce2424fb61da2f844a84219"],"layout":"IPY_MODEL_042a04c16c0d4aee9433658b9d3cfbb7"}},"28b93428e53044bd884b698dc966c484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c483230eabb40afa9b7b410fad487d7","placeholder":"​","style":"IPY_MODEL_9f97177644ff4ce6adeb858e0830c086","value":"Downloading pytorch_model.bin: 100%"}},"3f984ce8655144be81b4a842da06d507":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_355e5da46bf54bb6a3e8e4c1e82049c6","max":510410941,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfb294c852e94afd8e24b3296c7d5bb0","value":510410941}},"d0b349918ce2424fb61da2f844a84219":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_010cd836077a4a4fbd4e5aaf824f15be","placeholder":"​","style":"IPY_MODEL_8fda36eca9c849ca90fabc0f5843d411","value":" 510M/510M [00:05&lt;00:00, 85.3MB/s]"}},"042a04c16c0d4aee9433658b9d3cfbb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c483230eabb40afa9b7b410fad487d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f97177644ff4ce6adeb858e0830c086":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"355e5da46bf54bb6a3e8e4c1e82049c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfb294c852e94afd8e24b3296c7d5bb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"010cd836077a4a4fbd4e5aaf824f15be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fda36eca9c849ca90fabc0f5843d411":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bab1447b5db048748182b65457429901":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81db9a1051194faf9b10cddfd632a2b6","IPY_MODEL_7372c8c4105d4ed78ad22b70260a7d00","IPY_MODEL_85db9c6dced9484d94b5bf8ec46f4e5f"],"layout":"IPY_MODEL_e5544f7da9884244bcef3b2b87852eac"}},"81db9a1051194faf9b10cddfd632a2b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1631c03ed99d478eb562cee893e0f181","placeholder":"​","style":"IPY_MODEL_f2738ce1dce349d6b3fcd1dfb9ff8873","value":"Downloading (…)neration_config.json: 100%"}},"7372c8c4105d4ed78ad22b70260a7d00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_206c6e2dee5f4dc5b8384d2ae26dee4c","max":119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_703833a8526042c48f4813fefd992099","value":119}},"85db9c6dced9484d94b5bf8ec46f4e5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc6238d4b6284a029d882cb0c8bdc335","placeholder":"​","style":"IPY_MODEL_a9fc8c0b135046bdacfe7f06095935a0","value":" 119/119 [00:00&lt;00:00, 10.2kB/s]"}},"e5544f7da9884244bcef3b2b87852eac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1631c03ed99d478eb562cee893e0f181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2738ce1dce349d6b3fcd1dfb9ff8873":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"206c6e2dee5f4dc5b8384d2ae26dee4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"703833a8526042c48f4813fefd992099":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc6238d4b6284a029d882cb0c8bdc335":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9fc8c0b135046bdacfe7f06095935a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}